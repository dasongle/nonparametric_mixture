% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% other packages
\usepackage{../Definitions}
\newcommand{\hmu}{\widehat{\mu}}
\newcommand{\hCcal}{\widehat{\Ccal}}
\newcommand\tl{\tilde}
\input{../tensor-macros}

\usepackage{color}
\newcommand{\Note}[1]{{\color{red}{\bf\sf [note: #1]}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Nonparametric Estimation of Multi-view Latent Variable Models}

\begin{document}

\twocolumn[
\icmltitle{Nonparametric Estimation of Multi-View Latent Variable Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Le Song, Anima, B. Dai and B. Xie}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Kernel Methods, Latent Variable Models, Spectral Algorithms}

\vskip 0.3in
]

\begin{abstract}
		Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixture of discrete or Gaussian distributions. In this paper, we propose a kernel method for estimating the parameters of multi-view latent variable models, allowing each mixture component to be nonparametric.
		The key idea of the method is to embed joint distribution of multi-view latent variable into the reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. Furthermore, we provide theoretical guarantee for the proposed method and show that it compares favorably to EM algorithm and other existing spectral algorithms in our experiments.
\end{abstract}

\section{Introduction} \label{sec:intro}

\setlength{\abovedisplayskip}{4pt}
\setlength{\abovedisplayshortskip}{1pt}
\setlength{\belowdisplayskip}{4pt}
\setlength{\belowdisplayshortskip}{1pt}
\setlength{\jot}{3pt}

%\setlength{\floatsep}{1ex}
\setlength{\textfloatsep}{3ex}

Latent variable models have been used to address various machine learning
problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis~\citep{RabJua86,Clark90,HofRafHan02,BleNgJor03}.
Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models~\citep{HsuKakZha09,ParSonXin11,SonParXin11,FosRodUng12,more}. Compared to Expectation-Maximization (EM) algorithms \citep{DemLaiRub77} traditionally used for the same task, spectral algorithms are better in terms of their computational efficiency and provable guarantees.

However, current spectral algorithms are largely restricted to mixture
of discrete or Gaussian distributions. When the mixture components are distributions other than these standard distributions, the theoretical guarantees for these algorithms no longer applies, and the performance of these algorithms can be very poor in experiments. A nonparametric estimation of the parameters of latent variables with provable guarantee is still missing from the literature.

In this paper, we propose a kernel method for estimating the parameters of multi-view latent variables where the mixture component can be nonparametric.

We will kernel embedding of distributions, covariance operators and tensor power methods.

We provide both theoretical guarantee for our method and empirical evidence of the method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We denote by $X$ a random variable with domain $\Xcal$,
and refer to instantiations of $X$ by the lower case character, $x$.
We endow $\Xcal$ with some $\sigma$-algebra $\Ascr$ and denote a distributions (with respect to $\Ascr$) on $\Xcal$ by $\PP(X)$. We will also deal with multiple random variables, $X_1, X_2, \ldots, X_{\ell}$, with joint distribution $\PP(X_1,X_2,\ldots,X_{\ell})$. For simplicity of notation, we assume that the domains of all $X_t, t \in [\ell]$ are the same, but the methodology applies to the cases where they have different domains. Furthermore, we denote by $H$ hidden variables with domain $\Hcal$ and distribution $\PP(H)$.

A \emph{reproducing kernel Hilbert space (RKHS)} $\Fcal$ on $\Xcal$ with a kernel $k(x,x')$ is a Hilbert space of
functions $f(\cdot):\Xcal \mapsto \RR$ with inner product $\inner{\cdot}{\cdot}_{\Fcal}$. Its element $k(x,\cdot)$ satisfies the reproducing property:
$\inner{f(\cdot)}{k(x, \cdot)}_{\Fcal} = f(x)$, and consequently, $\inner{k(x,\cdot)}{k(x', \cdot)}_{\Fcal} = k(x,x')$,
meaning that we can view the evaluation of a function $f$ at any point $x\in\Xcal$ as an inner product. Alternatively, $k(x,\cdot)$ can  be viewed as an implicit feature map $\phi(x)$ where $k(x,x')=\inner{\phi(x)}{\phi(x')}_{\Fcal}$.
Popular kernel functions on $\RR^n$ include the polynomial kernel $k(x,x') =
(\inner{x}{x'}+c)^d$ and the Gaussian RBF kernel $k(x,x') = \exp(-s
  \nbr{x-x'}^2)$. Kernel functions have also been defined on
graphs, time series, dynamical systems, images and other structured
objects \, \cite{SchTsuVer04}. Thus the methodology presented below can  readily be generalized to a diverse range of data types as long as kernel functions are defined for them. Similarly, we denote by $\Gcal$ an RKHS on $\Hcal$ with kernel $l(h,h')$, and by $\psi(h)$ the corresponding feature map.

% Furthermore, we let $h \in [k]$ be a discrete random variable with $P(h =
% j) = \pi_j$ for all $j \in [k]$.
%
% We denote by $X$ a random variable with domain $\Xcal$ and distribution $P(X)$, and refer to instantiations of $X$ by the lower case character, $x$.
% We will focus on continuous domains, and denote the corresponding density by $p(X)$. We will also deal with multiple random variables, $X_1, X_2, \ldots, X_{\ell}$, with joint distribution $P(X_1,X_2,\ldots,X_{\ell})$. For simplicity of notation, we assume that the domain of all $X_t, t \in [\ell]$ also be $\Xcal$, but the methodology applies to the cases where they have different domains. Furthermore, we let $H \in [k]$ be a discrete random variable with $P(H =
% j) = \pi_j$ for all $j \in [k]$.



\section{Kernel Embedding of Distributions}
\label{sec:embedding}

We begin by providing an overview of kernel embeddings of distributions, which are \emph{implicit} mappings of distributions into potentially \emph{infinite} dimensional RKHS.\footnote{By ``implicit'', we mean that we do not need to explicitly construct the feature spaces, and the actual computations boil down to kernel matrix operations.} The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function \, \cite{SmoGreSonSch07,SriGreFukLanetal08},
\begin{align}
  \mu_{X} \, := \, \EE_{X} \sbr{\phi(X)} \, = \, \int_{\Xcal} \phi(x) \, \PP(dx),  \label{eq:embedding}
\end{align}
where the distribution is mapped to its expected feature map,~\ie,~to a point in a potentially infinite-dimensional and implicit feature space.
 The kernel embedding $\mu_{X}$ has the property that the expectation of any RKHS function $f$ can be evaluated as an inner product in $\Fcal$,
$
  \EE_{X} [f(X)] = \inner{\mu_{X}}{f}_{\Fcal}, \, \forall f\in\Fcal.
$

Kernel embeddings can be readily generalized to joint distributions of two or more variables using tensor product feature maps. For instance, we can embed a joint distribution of two variables $X_1$ and $X_2$ into a tensor product feature space $\Fcal\times \Fcal$ by
\begin{align}
    \Ccal_{X_1X_2} \, &:= \, \EE_{X_1X_2}[\phi(X_1)\otimes \phi(X_2)] \\
    \, &= \, \int_{\Xcal \times \Xcal} \phi(x_1) \otimes \phi(x_2) \, \PP(dx_1 \times dx_2)
\end{align}
where the reproducing kernel for the tensor product features satisfies
$
	\inner{\phi(x_1)\otimes \phi(x_2) }{\phi(x_1')\otimes \phi(x_2') }_{\Fcal\times \Fcal} \, = \,  k(x_1,x_1')\,k(x_2,x_2').
$

Kernel embedding of distributions have both rich representational power. The mapping is injective for characteristic kernels~\cite{SriGreFukLanetal08}. That is, if two distributions, $\PP(X)$ and $\QQ(X)$, are different, they will be mapped to two distinct points in the RKHS. For domain $\RR^d$, many commonly used kernels are characteristic, such as the Gaussian RBF kernel $\exp(-\sigma \|x - x'\|^2)$ and Laplace kernel $\exp(-\sigma \|x - x'\|)$.
This injective property of kernel embeddings has been exploited to design  state-of-the-art two-sample tests~\cite{GreBorRasSchetal12} and  independence tests~\cite{GreFukTeoSonetal08}.

{\bf Kernel embeddings as multilinear operators.}
The joint embeddings can also be viewed as an uncentered cross-covariance operator $\Ccal_{X_1X_2}:\Fcal\mapsto \Fcal$ by the standard equivalence between a tensor product feature and a linear map.
That is, given two functions $f_1,f_2\in\Fcal$, their covariance can be computed by
$
    \EE_{X_1X_2}[f_1(X_1)f_2(X_2)]=\inner{f_1}{\Ccal_{X_1X_2} f_2}_{\Fcal}
$
, or equivalently
$
\inner{f_1\otimes f_2}{\Ccal_{X_1X_2}}_{\Fcal\times\Fcal},
$
where in the former we view $\Ccal_{XY}$ as an operator while in the latter we view it as an element in tensor product feature space.
By analogy, $\Ccal_{X_1X_2X_3} := \EE_{X_1X_2X_3}[\phi(X_1)\otimes\phi(X_2)\otimes \phi(X_3)]$ can also be defined, which can be regarded as a multi-linear operator from $\Fcal\times\Fcal\times\Fcal$ to $\RR$.
It will be clear from the context whether we use $\Ccal_{XY}$ as an operator between two spaces or as an element from a tensor product feature space.

More generally, the kernel embedding $\Ccal_{X_{1:\ell}}$ for a joint distribution $\PP(X_1,X_2,\ldots,X_{\ell})$ can be viewed as a multi-linear operator (tensor) of order $\ell$ mapping from $\Fcal\times\ldots\times \Fcal$ to $\RR$. (For generic introduction to tensor and tensor notation, please see~\cite{KolBad09}). The operator is linear in each argument (mode) when fixing other arguments. Furthermore, the application of the operator to a set of elements $\cbr{f_i \in \Fcal}_{i \in [\ell]}$ can be defined using the inner product from the tensor product feature space,~\ie,
\begin{align}
	\Ccal_{X_{1:\ell}} \times_1 f_1 \times_2 \ldots \times_\ell f_\ell := 	\inner{\Ccal_{X_{1:\ell}}}{\;f_1\otimes\ldots\otimes f_\ell}_{\Fcal^d}=\EE_{X_{1:\ell}}\sbr{\prod_{i \in [\ell]} \inner{\phi(X_i)}{~f_i}_{\Fcal}},
\end{align}
where $\times_i$ means applying $f_i$ to the $i$-th argument of $\Ccal_{X_{1:\ell}}$. Furthermore, we can define the Hilbert-Schmidt norm $\nbr{\cdot}_{}$ of $\Ccal_{X_{1:\ell}}$ as
\[
 \nbr{\Ccal_{X_{1:\ell}}}_{}^2 = \sum_{i_1 = 1}^\infty \cdots \sum_{i_\ell = 1}^\infty \rbr{\Ccal_{X_{1:\ell}} \times_1 u_{i_1} \times_2 \ldots \times_\ell u_{i_\ell}}^2
\]
using a collection of orthonormal bases $\cbr{u_{i_1}}_{i_1=1}^\infty, \ldots, \cbr{u_{i_\ell}}_{i_\ell=1}^\infty$. We can also define the inner product for the space of such operator that $\nbr{\Ccal_{X_{1:\ell}}}_{}<\infty$ as
\begin{align}
	\inner{\Ccal_{X_{1:\ell}}}{~\widetilde{\Ccal}_{X_{1:\ell}}}_{}  =  \sum_{i_1 = 1}^\infty \cdots \sum_{i_\ell = 1}^\infty \rbr{\Ccal_{X_{1:\ell}} \times_1 u_{i_1} \times_2 \ldots \times_\ell u_{i_\ell}}\, \rbr{\widetilde{\Ccal}_{X_{1:\ell}} \times_1 u_{i_1} \times_2 \ldots \times_\ell u_{i_\ell}}.
\end{align}
% When $\Ccal_{X_{1:\ell}}$ has the form of $\EE_{X_{1:\ell}}\sbr{\phi(X_1)\otimes\ldots\otimes \phi(X_\ell)}$, the above inner product reduces to $\EE_{X_{1:\ell}}[\widetilde{\Ccal}_{X_{1:\ell}} \times_1 \phi(X_1) \times_2 \ldots \times_\ell \phi(X_\ell)]$.


The joint embedding, $\Ccal_{X_1 X_2}$, is a 2nd order tensor, and we can essentially use notation and operations for matrices. For instance, we can perform singular value decomposition
\[
    \Ccal_{X_1X_2} = \sum_{i=1}^{\infty} \sigma_i \cdot u_{i_1} \otimes u_{i_2}
\]
where $\sigma_i \in \RR$ are singular values ordered in nonincreasing manner, and $\cbr{u_{i_1}}_{i_1=1}^{\infty} \subset \Fcal, \cbr{u_{i_2}}_{i_2=1}^{\infty} \subset \Fcal$ are singular vectors and orthonormal bases. The rank of $\Ccal_{X_1X_2}$ is the smallest $k$ such that $\sigma_i = 0$ for $i > k$.

{\bf Example 1.} The probability vector of a discrete variable $X \in [n]$, and the joint probability table of two discrete variables $X_1 \in [n]$ and $X_2 \in [n]$, are both kernel embeddings. To see this, let the kernel be the Kronecker delta kernel $k(x,x') = \delta(x,x')$. The corresponding feature map $\phi(x)$ is then the standard basis of $e_{x}$ in $\RR^n$. Then
\begin{align}
    \mu_X
		= \EE_X[e_X] = \rbr{
      \begin{array}{c}
         \PP(x = 1) \cr
         \vdots \cr
         \PP(x = n)
       \end{array}
    },~
		\Ccal_{X_1X_2}
		= \EE_{X_1X_2}[e_{X_1} \otimes e_{X_2}]=
		\rbr{
        \begin{array}{c}
            \cr
            \PP(x_1=s,x_2=t) \cr
						\cr
        \end{array}
    }_{s,t \in [n]}. \label{eq:jointprobabilitytable}
\end{align}

{\bf Finite sample estimate.} While we rarely have access to the true underlying distribution, $\PP(X)$,
we can readily estimate its embedding using a finite sample average. Given a sample $\Dcal_{X} = \cbr{x^1, \ldots, x^m}$ of size $m$ drawn~\iid~from $\PP(X)$, the empirical kernel embedding is
\begin{align}
    \hmu_{X} &:= \frac{1}{m} \sum\nolimits_{i=1}^m \phi(x^i). \label{eq:empirical_embedding}
\end{align}
This empirical estimate converges to its population counterpart in RKHS norm, $\|\hmu_X - \mu_X \|_{\Fcal}$, with a rate of $O_p(m^{-\frac{1}{2}})$~\cite{SmoGreSonSch07}. We note that this rate is independent of the dimension of $X$, meaning that statistics based on kernel embeddings circumvent the curse of dimensionality.

Kernel embeddings of joint distributions inherit the previous two properties of general embeddings: injectivity and easy empirical estimation. Given $m$ pairs of training examples $\Dcal_{XY}=\cbr{(x_1^i,x_2^i)}_{i\in [m]}$ drawn~\iid~from $\PP(X_1,X_2)$,
the covariance operator can then be estimated as
\begin{align}
 \widehat \Ccal_{X_1X_2} =\frac{1}{m}\sum_{i=1}^m \phi(x_1^i) \otimes \phi(x_2^i). \label{eq:empirical_covariance}
\end{align}

By virtue of the kernel trick, most of the computation required for statistical inference using kernel embeddings can be reduced to the Gram matrix manipulation. The entries in the Gram matrix $K$ correspond to the kernel value between data points $x^i$ and $x^j$,~\ie,~$K_{ij} = k(x^i,x^j)$, and therefore its size is determined by the number of data points in the sample. The size of the Gram matrix is in general much smaller than the dimension of the feature spaces (which can be infinite). This enables efficient nonparametric methods using the kernel embedding representation. If the sample size is large, the computation in kernel embedding methods may be expensive. In this case, a popular solution is to use a low-rank approximation of the Gram matrix, such as incomplete Cholesky factorization~\cite{FinSch01}, which is known to work very effectively in reducing computational cost of kernel methods, while maintaining the approximation accuracy.

\noindent {\bf Relation between kernel embedding and the density function.} Basically kernel embeddings maps the density to a function in the RKHS. See Steinwardt and Vert (need to say more).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiview Latent Variable Models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Multi-view latent variable models (\eg, \, na\"ive Bayes models) are a
special class of Bayesian networks in which observed variables $X_1, X_2,
\ldots, X_\ell$ are conditionally independent given a latent variable $H$, and
the conditional distributions, $\PP(X_t|H)$, of the $X_t, t \in [\ell]$ given the hidden variable $H$ can be different. The conditional independent structure of a multiview latent variable model can be found in Figure~\ref{fig:graphical-model}(a), and many complicated graphical models, such as the hidden Markov model in Figure~\ref{fig:graphical-model}(b), can be reduced to a multiview latent variable model.

For simplicity of exposition, we now consider a simple model with three observed variables, $X_1, X_2$ and $X_3$ which are conditionally independent given $H$, and furthermore the conditional distributions, $\PP(X_t|H)$, are the same for $t \in \cbr{1,2,3}$. That is the random variables are {\em exchangeable}. Our analysis can be easily extended to the general multi-view setting, see~\cite{AnandkumarEtal:tensor12} for details. Then the joint distributions, $\PP(X_1,X_2)$ and $\PP(X_1,X_2,X_3)$, can be factorized respectively as
\begin{align}
 \PP(dx_1 \times dx_2) &= \int_{\Hcal}\, \PP(dx_1 | h )\, \PP(d x_2 | h)\, \PP(dh) \\
  \PP(dx_1 \times dx_2\times dx_3) &= \int_{\Hcal}\, \PP(dx_1 | h )\, \PP(dx_2 | h)\, \PP(dx_3 | h)\, \PP(dh),
\end{align}
and the corresponding kernel embeddings can be factorized respectively as
\begin{align}
  \Ccal_{X_1X_2}
  &= \int_{\Xcal\times \Xcal \times \Hcal}\, \phi(x_1) \otimes \phi(x_2)\, \PP(dx_1 | h )\, \PP(dx_2 | h)\, \PP(dh) \\
  &= \int_{\Hcal} \rbr{\int_\Xcal \phi(x_1)\, \PP(dx_1|h)} \otimes \rbr{\int_\Xcal \phi(x_2)\, \PP(dx_2 |h)} \PP(dh) \\
  &= \int_{\Hcal} \rbr{\Ccal_{X|H} \psi(h)} \otimes \rbr{\Ccal_{X|H} \psi(h)}\, \PP(dh) \\
  &= \Ccal_{X|H}\, \rbr{\int_{\Hcal} \psi(h) \otimes \psi(h)\, \PP(dh)}\, \Ccal_{X|H}^\top\\
  &= \Ccal_{X|H}\, \Ccal_{HH}\, \Ccal_{X|H}^\top \\
%   &= \Ccal_{HH} \times_1 \Ccal_{X|H} \times_2 \Ccal_{X|H} \\
  \Ccal_{X_1 X_2 X_3}
  &= \int_{\Xcal\times \Xcal \times \Xcal \times \Hcal}\, \phi(x_1) \otimes \phi(x_2) \otimes \phi(x_3) \, \PP(dx_1 | h )\, \PP(dx_2 | h)\, \PP(dx_3 | h)\,  \PP(dh) \\
  &= \Ccal_{HHH} \times_1 \Ccal_{X|H} \times_2 \Ccal_{X|H} \times_3 \Ccal_{X|H}
\end{align}

{\bf $k$-restricted factorization.} If we use $k$-restricted conditional embedding operator to form approximations of the above operators, then we have
\begin{align}
  \widetilde \Ccal_{X_1X_2} &:= \widetilde \Ccal_{X|H}\, (\Vcal^\top \Ccal_{HH} \Vcal)\, \widetilde \Ccal_{X|H}^\top \\
  \widetilde \Ccal_{X_1X_2X_3} &:= \rbr{\Ccal_{HHH} \times_1 \Vcal^\top \times_2 \Vcal^\top \times_3 \Vcal^\top} \times_1 \widetilde \Ccal_{X|H} \times_2 \widetilde \Ccal_{X|H} \times_3 \widetilde \Ccal_{X|H}
\end{align}
Let the singular value decomposition of $\Ccal_{HH}$ be $\sum_{i=1}^{\infty} \sigma_i \cdot v_i \otimes v_i$, and we construct $\Vcal$ using the leading $k$ singular vectors. Then $\Vcal^\top \Ccal_{HH} \Vcal$ is a diagonal matrix with entries
\begin{align}
 \Vcal^\top \Ccal_{HH} \Vcal = \rbr{
  \begin{array}{cccc}
    \sigma_1 & 0 & \ldots & 0 \cr
    0 & \sigma_2 & \ldots & 0 \cr
    \vdots & \vdots & \ddots & \vdots \cr
    0 & 0 & \ldots & \sigma_k
  \end{array}
 }
\end{align}
where we used $\int_{\Hcal} v_i(h)\, v_{i'}(h)\, \PP(dh) = 0$ for $i\neq i'$.
But the 3rd order tensor
\begin{align}
 \Ccal_{HHH} \times_1 \Vcal^\top \times_2 \Vcal^\top \times_3 \Vcal^\top
	& = \EE_{H} \sbr{ (\Vcal^\top \psi(h)) \otimes (\Vcal^\top \psi(h)) \otimes (\Vcal^\top \psi(h))}  \\
	& = \rbr{
	\begin{array}{c}
	  \cr
	  \int_{\Hcal} v_i(h)\, v_{i'}(h)\, v_{i''}(h)\, \PP(dh) \cr
	  \cr
	\end{array}
	}_{i,i',i'' \in [k]}
\end{align}
is not a diagonal tensor in general. However, there are important special cases where both the 2nd and 3rd order tensors are simultaneously diagonalizable.
% We can assume that $\Ccal_{HHH}$ is approximately diagonalizable by $\Vcal$, that is
% \begin{align}
%  	a := \sum_{i,i',i''~\text{not all equal}} \rbr{\int_{\Hcal} v_i(h)\, v_{i'}(h)\, v_{i''}(h)\, \PP(dh)}^2
% \end{align}
% is negligible in size.

{\bf Discrete latent variable.} When the hidden variable $H \in [k']$ is discrete, and we use Kronecker delta kernel $\delta(h,h')$ on $\Hcal$, then both
\begin{align}
 \Ccal_{HH} = \rbr{
  \begin{array}{ccc}
    \PP(h=1) & \ldots & 0 \cr
    \vdots & \ddots & \vdots \cr
    0 & \ldots & \PP(h=k')
  \end{array}
 },
 ~\text{and}~
 \Ccal_{HHH} = \rbr{
  \begin{array}{c}
    \cr
    \PP(h=i) \delta(i,i') \delta(i',i'') \cr
    \cr
  \end{array}
 }_{i,i',i''\in[k']}
\end{align}
are diagonal tensors. The operator $\Vcal$ used for constructing $k$-restricted conditional embedding operator can be contructed from the standard basis. That is $\Vcal = \sum_{i \in [k]} e_i \otimes \tilde e_i$, where $e_i$ and $\tilde e_i$ are the $i$-th standard basis vector in $\RR^{k'}$ and $\RR^k$ respectively.

% and taking value in $[k]$, let
% \[
%  P_j(X_t) \ :=\ P(X_t\, |\, H = j),\quad j \in [k],\ t \in \{1,2,3\}
% \]
% We first note that the distribution factorizes as follows
% \begin{align}
% P( X_t, X_{t'} )
% \ & =\ \sum_{j=1}^k \pi_j \cdot P_j(X_t) \cdot P_j(X_{t'}),
% \quad \{t,t'\} \subset \{1,2,3\} ,\ t \neq t' \label{eq:joint2} \\
% P(X_1, X_2, X_3)
% \ & =\ \sum_{j=1}^k \pi_j \cdot P_j(X_1) \cdot P_j(X_2) \cdot P_j(X_3). \label{eq:joint3}
% \end{align}

%\begin{figure}[t]
%\begin{center}
%\subfigure[Multi-view models]{\label{fig:exchangeable}
%\begin{tikzpicture}
%  [
%    scale=1.0,
%    observed/.style={circle,minimum size=0.7cm,inner sep=0mm,draw=black,fill=black!20},
%    hidden/.style={circle,minimum size=0.7cm,inner sep=0mm,draw=black},
%  ]
%  \node [hidden,name=h] at ($(0,0)$) {$H$};
%  \node [observed,name=x1] at ($(-1.5,-1)$) {$X_1$};
%  \node [observed,name=x2] at ($(-0.5,-1)$) {$X_2$};
%  \node at ($(0.5,-1)$) {$\dotsb$};
%  \node [observed,name=xl] at ($(1.5,-1)$) {$X_\ell$};
%  \draw [->] (h) to (x1);
%  \draw [->] (h) to (x2);
%  \draw [->] (h) to (xl);
%\end{tikzpicture}}
%\hfil\subfigure[Hidden Markov model]{\label{fig:hmm}
%\begin{tikzpicture}
%  [
%    scale=1.0,
%    observed/.style={circle,minimum size=0.7cm,inner sep=0mm,draw=black,fill=black!20},
%    hidden/.style={circle,minimum size=0.7cm,inner sep=0mm,draw=black},
%  ]
%  \node [hidden,name=h1] at ($(-1.2,0)$) {$H_1$};
%  \node [hidden,name=h2] at ($(0,0)$) {$H_2$};
%  \node [name=hd] at ($(1.2,0)$) {$\dotsb$};
%  \node [hidden,name=hl] at ($(2.4,0)$) {$H_\ell$};
%  \node [observed,name=x1] at ($(-1.2,-1)$) {$X_1$};
%  \node [observed,name=x2] at ($(0,-1)$) {$X_2$};
%  \node [observed,name=xl] at ($(2.4,-1)$) {$X_\ell$};
%  \draw [->] (h1) to (h2);
%  \draw [->] (h2) to (hd);
%  \draw [->] (hd) to (hl);
%  \draw [->] (h1) to (x1);
%  \draw [->] (h2) to (x2);
%  \draw [->] (hl) to (xl);
%\end{tikzpicture}}
%\end{center}
%\vspace{-5mm}
%\caption{Examples of latent variable models.
%}
%\label{fig:graphical-model}
%\vspace{-1mm}
%\end{figure}

Furthermore, in this case, $\widetilde \Ccal_{X|H} = \rbr{\mu_{X|h=1},\ \mu_{X|h=2},\ \ldots,\ \mu_{X|h=k}}$, and the approximate kernel embeddings for $\PP(X_1,X_2)$ and $\PP(X_1,X_2,X_3)$ are
\begin{align}
  \widetilde \Ccal_{X_1 X_2}
  & = \sum_{h \in [k]} \pi_h \cdot \mu_{X|h} \otimes \mu_{X|h}, \label{eq:joint2} \\
  \widetilde \Ccal_{X_1 X_2 X_3}
  &= \sum_{h \in [k]} \pi_h \cdot \mu_{X|h} \otimes \mu_{X|h} \otimes \mu_{X|h} \label{eq:joint3}
\end{align}
where $\pi_h:=\PP(h)$.


% Then we can embedding into RKHS the second and third order marginal distribution of the observed variables in the multiview latent variable models in \, \eq{eq:joint2} and \, \eq{eq:joint3}. These kernel embeddings will also factorize according the conditional independence structure of the underlying model. More specifically,
% \begin{align}
%   \Ccal_{X_1,X_2,X_3}
%   = \EE_{X_1,X_2,X_3}[\phi(X_1)\otimes \phi(X_2) \otimes \phi(X_3)]
% \end{align}
% Making use of the distribution factorization of the model, we have that
% \begin{align}
%   \Ccal_{X_1,X_2,X_3} =
%   \sum_{j=1}^k \pi_j \cdot \EE_{X_1| H=j}[\phi(X_1)]\otimes \EE_{X_2|H=j} [\phi(X_2)] \otimes \EE_{X_3|H=j}[\phi(X_3)]
% \end{align}
% where $\mu_{X_t|j}:=\EE_{X_t| H=j}[\phi(X_t)] = \int_{\Xcal} \phi(X_t)\, d P(X_t|H=j)$ is the
% the conditional embedding of the observed variable $X_t$ given latent variable $H=j$.
% Then we can express the embedding $\Ccal_{X_1,X_2,X_3}$ as a sum of outer product of conditional embeddings, \, \ie,
% \begin{align}
%   \Ccal_{X_1,X_2,X_3}
%   = \sum_{j=1}^k \pi_j \cdot \mu_{X_1|j} \otimes \mu_{X_2|j} \otimes \mu_{X_3|j}
% \end{align}
% Similarly, for the joint distribution of the two observed variable in \, \eq{eq:joint2}, we have
% \begin{align}
%   \Ccal_{X_t,X_{t'}}
%   = \sum_{j=1}^k \pi_j \cdot \mu_{X_t|j} \otimes \mu_{X_{t'}|j},\quad \{t,t'\} \subset \{1,2,3\},\ t\neq t'
% \end{align}

{\bf Identifiability.} Allman et al. showed that, under mild conditions, a finite mixture of nonparametric product distributions is identifiable. The multiview latent variable model in~\eq{eq:joint3} has the same form as a finite mixture of nonparametric product distribution, and therefore we can adapt Allman's results to the current setting.
\begin{theorem}
  Let $\PP(X_1,X_2,X_3)$ be a multiview latent variable model of the form~\eq{eq:joint3}, such that the conditional distributions $\cbr{\PP(X|h)}_{h \in [k]}$ are linearly independent. Then, the set of parameters $\cbr{\pi_h, \mu_{X|h}}_{h \in [k]}$ are identifiable from $\Ccal_{X_1 X_2 X_3}$, up to label swapping of the hidden variable $H$.
\end{theorem}

% \section{Whitening with Three Different Views}

% \begin{itemize}
% 	\item $\widetilde{\Ccal}_{12} = \Ccal_{12} \Ucal_2^\top$; $\widetilde{\Ccal}_{13} = \Ccal_{13} \Ucal_3^\top$; $\widetilde{\Ccal}_{23} = \Ucal_{2} \Ccal_{23} \Ucal_3^\top$
% 	\item $\widetilde{\Ccal}_{11} = \widetilde{\Ccal}_{12} (\widetilde{\Ccal}_{32})^{-1} \widetilde{\Ccal}_{31}^\top = \widetilde{\Ucal}_1 \widetilde{\Scal}_1 \widetilde{\Ucal}_1^\top$
% 	\item $\widetilde{\Tcal} = \Ccal_{123} \times_1 (\widetilde{\Ucal}_1 \widetilde{\Scal}_1^{-1/2})^\top  \times_2 (\widetilde{\Ucal}_2 \widetilde{\Scal}_2^{-1/2})^\top \times_3 (\widetilde{\Ucal}_3 \widetilde{\Scal}_3^{-1/2})^\top$
% \end{itemize}

\section{Kernel Algorithm}

We will deal with the discrete latent variable case.
The parameters $\cbr{\pi_h, \mu_{X|h}}_{h \in [k]}$, of the multiview latent variable model can be recovered from $\Ccal_{X_1 X_2}$ and $\Ccal_{X_1 X_2 X_3}$ using the following simple algorithm.
\begin{enumerate}
  \item Eigen-value decomposition for $\Ccal_{X_1 X_2}$,
    $$\Ccal_{X_1 X_2} = \sum_{i=1}^{\infty} \sigma_i \cdot u_i \otimes u_i$$
    where the eigen-values are ordered in non-decreasing manner. Let the leading $k$ eigenvectors corresponding to the largest $k$ eigen-value be  $\Ucal_k:=(u_1,u_2,\ldots,u_k)$, and the eigen-value matrix of $S_k:=\diag(\sigma_1,\sigma_2,\ldots,\sigma_k)$.
  \item Whiten the 3rd order kernel embedding $\Ccal_{X_1 X_2 X_3}$ using whitening matrix $\Wcal:= \Ucal_k S_k^{-1/2}$.
    $$\Tcal := \Ccal_{X_1 X_2 X_3} \times_1 (\Wcal^\top) \times_2 (\Wcal^\top) \times_3 (\Wcal^\top)$$
    where $\times_i$ denotes mode-$i$ tensor-matrix multiplication.
  \item Find the leading $k$ tensor eigenvectors $V_k$ for $\Tcal$ using tensor power method.
  \item Recover the conditional embedding operator by
    $$ \Ccal_{X|H} = (\mu_{X|h=1},\mu_{X|h=1},\ldots,\mu_{X|h=k}) = (\Wcal)^\dagger V_k $$
\end{enumerate}

{\bf Finite sample estimate.} Given $m$ observation $\Dcal_{X_1 X_2 X_3}=\{(x_1^i,x_2^i,x_3^i)\}_{i \in [m]}$ drawn~\iid~from a multi-view latent variable model $\PP(X_1,X_2,X_3)$, we now design a kernel algorithm to estimate the latent parameters from data. Although the empirical kernel embeddings has infinite dimensions, we can carry out the decomposition using just the kernel matrices.

\begin{enumerate}

\item We will perform a kernel eigenvalue decomposition of the empirical 2nd order embedding
$$
  \widehat \Ccal_{X_1 X_2}:= \frac{1}{2m} \sum_{i=1}^{m} \rbr{\phi(x_1^i) \otimes \phi(x_2^i) + \phi(x_2^i) \otimes \phi(x_1^i)}.
$$
Denote the implicitly formed feature matrix by
\begin{align}
  \Phi &:= (\phi(x_1^1),  \phi(x_1^2), \ldots, \phi(x_1^m), \phi(x_2^1),  \phi(x_2^2), \ldots, \phi(x_2^m))  \\
  \Psi &:= (\phi(x_2^1), \phi(x_2^2), \ldots, \phi(x_2^m), \phi(x_1^1),  \phi(x_1^2), \ldots, \phi(x_1^m))
\end{align}
respectively, and the corresponding kernel matrix be $K = \Phi^\top \Phi$ and $L = \Psi^\top \Psi$. Using the feature matrix, $\Ccal_{X_1 X_2}$ can be expressed as
$$
	\Ccal_{X_1 X_2} = \frac{1}{2m} \Phi \Psi^\top.
$$
Its leading $k$ eigenvectors $\Ucal_k = (u_1,\ldots,u_k)$ will lie in the span of the column of  $\Phi$,~\ie,~$\Ucal_k = \Phi (\beta_1,\ldots,\beta_k)$ where $\beta \in \RR^m$. Then we can transform the eigen-value decomposition problem for an infinite dimensional matrix to a problem involving finite dimensional kernel matrices,
$$
	\Ccal_{X_1 X_2}\, \Ccal_{X_1 X_2}^\top\, u = \sigma \;u
	\quad \Rightarrow \quad
	\frac{1}{4m^2}\Phi \Psi^\top \Psi \Phi^\top \Phi \beta = \sigma\,\Phi \beta
	\quad \Rightarrow \quad
	\frac{1}{4m^2} K L K \beta = \sigma \,K \beta.
$$
Let the Cholesky decomposition of $K$ be $R^\top R$. Then by redefining $\widetilde{\beta}=R\beta$, and solving an eigenvalue problem
\begin{align}
 \frac{1}{4m^2} R L R^\top \widetilde{\beta} = \sigma \, \widetilde{\beta},~~\text{and obtain}~\beta = R^{\dagger} \widetilde{\beta}.
\end{align}
The resulting eigenvectors satisfy $u_i^\top u_{i'} = \beta_i^\top \Phi^\top \Phi \beta_{i'} =  \beta_{i}^\top K  \beta_{i'} =  \widetilde{\beta}_{i}^\top \widetilde{\beta}_{i'}=\delta_{ii'}$.
This algorithm is summarized in Algorithm~\ref{alg:svd}.

\begin{algorithm}[t!]
\caption{KernelSVD($K$, $L$, $k$)}
% 	\textbf{In}: Two kernel matrices $K$ and $L$, and desired rank $r$ \\
	\textbf{Out}: $S_k$ and $(\beta_1,\ldots,\beta_k)$\\[-0.4cm]
  \begin{algorithmic}[1]
    \STATE Perform Cholesky decomposition:\ $K=R^\top R$
    \STATE Solve eigen-decomposition problem:\ $\frac{1}{4m^2} R L R^\top \widetilde{\beta} = \sigma\,\widetilde{\beta}$
    \STATE Let the $k$ leading eigen-values be:\ $S_k = \diag(\sigma_1,\ldots,\sigma_k)$
    \STATE Let the corresponding $k$ leading eigenvectors be:\ $(\widetilde{\beta}_1,\ldots,\widetilde{\beta}_k)$
    \STATE Compute:\ $(\beta_1,\ldots,\beta_k) = R^\dagger (\widetilde{\beta}_1,\ldots,\widetilde{\beta}_k)$
% 		, and reorgnaize $(\thetab^1,\ldots,\thetab^n)^\top = (\beta_1,\ldots,\beta_k)$
  \end{algorithmic}
  \label{alg:svd}
\end{algorithm}

\item We whiten the empirical 3rd order embedding
$$
  \widehat \Ccal_{X_1 X_2 X_3}:=\frac{1}{3m}\sum_{i=1}^{m} \rbr{\phi(x_1^i) \otimes \phi(x_2^i) \otimes \phi(x_3^i) + \phi(x_3^i) \otimes \phi(x_1^i) \otimes \phi(x_2^i) + \phi(x_2^i) \otimes \phi(x_3^i) \otimes \phi(x_1^i)}
$$
to obtain
\begin{align}
  \widehat \Tcal := \frac{1}{3m}\sum_{i=1}^m \rbr{\xi(x_1^i) \otimes \xi(x_2^i) \otimes \xi(x_3^i) + \xi(x_3^i) \otimes \xi(x_1^i) \otimes \xi(x_2^i) + \xi(x_2^i) \otimes \xi(x_3^i) \otimes \xi(x_1^i)},
\end{align}
where
$$
	\xi(x_1^i) := S_k^{-1/2} (\beta_1,\ldots,\beta_k)^\top \Phi^\top \phi(x_1^i)\quad \in \quad \RR^k.
$$

\item We run tensor power method in Algorithm~\ref{alg:robustpower} on the finite dimension tensor $\widehat \Tcal$ to obtain its leading $k$ eigenvectors $V_k:=(v_1,\ldots,v_k)$.

\item The estimate for the conditional embedding is given by
\begin{align}
  \widehat \Ccal_{X|H} = (\widehat \mu_{X|h=1},\ldots, \widehat \mu_{X|h=k}) = \Phi (\beta_1,\ldots,\beta_k) S_k^{1/2} V_k
\end{align}

\end{enumerate}

\section{Interpretation with Parametric Family}

For interpretable results, we can project the nonparametric representation to parametric family of distributions (\eg, \, exponential families) as post-processing.

\input{../concentration-bounds}

\bibliographystyle{icml2014}
\bibliography{../nonparametric_mixture,../bibfile}

\end{document}