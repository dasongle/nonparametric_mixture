% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
% \usepackage{icml2014}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% other packages
\usepackage{../Definitions}
\newcommand{\hmu}{\widehat{\mu}}
\newcommand{\hCcal}{\widehat{\Ccal}}
\newcommand\tl{\widetilde}
\newcommand{\aacomment}[1]{\noindent{\textcolor{blue}{\textbf{\#\#\# AA:} \textsf{#1} \#\#\#}}}

% \newcommand{\bm}{\mathbf}
\input{../tensor-macros}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{exscale,relsize}
\usepackage{enumitem}

\usepackage{color}
\newcommand{\Le}[1]{{\color{red}{\bf\sf [note: #1]}}}
\newcommand{\bodai}[1]{{\color{violet}{\bf\sf [Dai remark: #1]}}}
\newcommand{\boxie}[1]{{\color{green}{\bf\sf [Bo Xie comment: #1]}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Nonparametric Estimation of Multi-view Latent Variable Models}

\begin{document}

\section*{Cover Letter}

Dear program chairs and reviewers,

We would like to acknowledge anonymous reviewers for their valuable comments on our paper ``nonparametric estimation of multi-view latent variable models''. We have revised it based on these suggestions. Please find our major changes and the answers to their questions below.


\section*{List of Major Changes}

\begin{itemize}
\item[1] We rewrite the introduction part to make our problem setting clear and emphasize the novelty.
\item[2] We explain the extraction form of conditional density from Hilbert space embedding.
\item[3] We add more discussions on the sample complexity. The bound for integral and the conditional density recovery error bound under unsupervised setting is derived based on the main theorem.
\item[4] We add discussion on generalizing our method to nonlinear independent component analysis and latent variable models with Dirichlet priors.
\item[5] We add more competitors, the spectral algorithm for mixture of spherical Gaussians (Hsu \& Kakade, 2013), EM-like algorithm for nonparametric model (Benaglia et al., 2009) and the recent nonparametric algorithm of Kasahara \& Shimotsu, 2010, on both synthetic datasets and real-world datasets.
\end{itemize}

\section*{To Meta-Reviwer}
A discussion about the difference between the proposed method to previous related works is added. We compare the proposed method to more competitors on both synthetic datasets and real-world datasets to strengthen the experiments.

\section*{To Reviewer 1}

\textbf{Novelty of the proposed model} 

Most previous work on kernel embedding of distributions, such as two sample tests and independence tests, do not consider the presence of latent variables. We consider novel aspect of the spectral properties of kernel embeddings and connect them to the spectral properties of infinite dimensional tensors and provide a sample complexity analysis. 

We are not aware of any prior work using kernel embeddings to explicitly recover the parameters of the latent variable model. The related work of Kernel embedding of hidden Markov models (Song et al. ICML 2010), and Kernel embedding of latent tree models (Song et al. NIPS 2011) focus on estimating the marginal density of the observed various but not the latent parameters. In contrast, our algorithm can recover the exact latent parameters, and we prove that the estimator is consistent and analyze its finite sample complexity in this work. 

\textbf{Novelty of technical analysis}

In terms of the analysis, previous work on tensor decomposition (Anandkumar Et al 2012) provided analysis when there is perturbation on the orthogonal tensor. In our current paper, we take into account the whitening perturbation and the sample bounds to achieve a certain level of concentration of the empirical moments and present a unified sample complexity analysis. This is novel. 

\textbf{Experimental details}

We've corrected the dataset number to 24 to be consistent. We \emph{only} split the dimensions but not the datasets. The dimensions are split according to the correlation between dimensions, so that weakly correlated dimensions go into different views.


\section*{To Reviewer 2}

The Hilbert space embedding of density is not only benefitting the calculation of integrals and expectations. A more important contribution is that providing another nonparametric form for modeling. Instead of making assumption about the density form, e.g., density in exponential family, the kernel embedding of density significantly relaxs this restrication and thus makes the model more flexiable. Actually, this advantage is obvious in our experiment where the parametric model performance worse with incorrect assumption. 


Regarding the form of the kernel function, any kernel suitable for kernel density estimation is also applicable for our method.
\bodai{this reviewer also questions about the kernel form saying `'you need a kernel which concentrates around x; think of a delta-distribution. So rule out polynomial kernels and, I guess, kernels on structures like graphs.''}

It should be aware that in our problem setting, we are not give the observations of $h$. This is an \emph{unsupervised} setting so that it is impossible to use kernel density estimation to recover the conditional density.

We add the extraction form of conditional density from Hilbert space embedding explicitly in main text. As you claimed, the Hilbert space embedding of density is efficiently in calculation of integrals, the MAP is very easy to estimated here. After we obtain the conditional distributions and prior $p(h)$, we could apply Bayes rule to find the posterior. In our setting, the conditional distributions are in the corresponding \emph{RKHS}, and thus given $h$, the observation $\{x_1,x_2,x_3\}$ has probability $p(x_1|h) = \langle\phi(x_1), \Phi \rangle A, p(x_2|h) = \langle\psi(x_1), \Psi \rangle B, p(x_1|h) = \langle\upsilon(x_1), \Upsilon \rangle C$\footnote{We denote $\Phi A, \Psi B, \Upsilon C$ as the conditional operators obtained via our algorithm.}

So, the normalizer could be computed as $Z = [p(H)\circ p(x_1|H)\circ p(x_2|H)\circ p(x_3|H)] \times \bm{1}$ where $\circ$ denotes element-wise product. And we have the posterior is
%
\begin{eqnarray*}
p(h|x_1, x_2, x_3) = \frac{p(h)\circ p(x_1|h)\circ p(x_2|h)\circ p(x_3|h)}{Z} \propto p(h)\circ p(x_1|h)\circ p(x_2|h)\circ p(x_3|h)
\end{eqnarray*}
%
We could select the $h$ with maximum $p(h|x_1, x_2, x_3)$. In our experiment, $Z$ is even no need to compute, 

\section*{To Reviewer 3}

\end{document}
